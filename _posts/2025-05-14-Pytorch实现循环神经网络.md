---
title: "Pytorch实现循环神经网络"
layout: single
date: 2025-05-14
toc: true
toc_sticky: true  # 可选：目录是否固定在侧边
categories: [ "Python"]
tags: ["DeepLearning"]
---

最近在看李沐老师的《动手学深度学习》，这是一本很好的深度学习入门教材，对于基础的神经网络知识的讲解相当到位。尤其是其中介绍循环神经网络的地方，老师的讲解和代码都很详细了。但是作为学生，在学习的过程中还是难免有不懂的地方，所以我想写这一篇博客来对这一章的内容进行一次小小的总结。

<div align="center">
  <img src="/assets/images/5.14/mild days.jpg" width="300">
  <p>我们羊文学也有自己的猫专了（）</p>
</div>

## 循环神经网络原理

循环神经网络是一种处理时序数据的方法，在 8.1 节中提到过两种方式：一是将前面一段时刻的数据全部作为输入，送给模型得到输出；二是定义一个隐状态，该隐状态汇集了前一段时间的信息，然后结合第 t 时刻的数据作为输入得到输出。前者为自回归模型（AR），后者为隐变量自回归模型（LVAR），显然循环神经网络属于后者。

> 然而似乎 RNN 并不严格属于隐变量自回归模型（）因为 RNN 的隐变量是确定的，一般的隐变量自回归模型都是需要从一个特定的分布中进行采样、积分或变分近似获得隐状态的。

- AR vs LVAR

| 方面          | 自回归模型（AR）                                  | 隐变量自回归模型（LVAR）                                                       |
| ----------- | ------------------------------------------ | -------------------------------------------------------------------- | 
| **模型形式**    | 显式建模：$$x_t = f(x_{t-1}, ..., x_{t-k})$$ | 引入隐变量：$$z_t \sim p(z_t)$$                                         
| **是否引入隐变量** | 没有隐变量，直接建模可观测变量                          | 每个时间步引入隐变量 $$z_t$$，建模潜在结构                                         |                                           |         |
| **建模能力**    | 限于直接可观测数据的表层模式                             | 可以捕捉复杂、非线性、抽象的时序结构                                                   |                                           |         |
| **代表模型**    | AR(p)、RNN、Transformer (标准训练方式)             | Variational RNN、Deep State Space Models、Latent Autoregressive Models |                                           |         |
| **训练难度**    | 相对较低，损失函数明确                                | 需要变分推断（如ELBO）、优化更复杂                                                  |                                           |         |
| **推理开销**    | 只需前几个 $$x$$ 值即可预测下一步                       | 需要同时推理 $$z_t$$，推理开销更大                                               |                                           |         |


关于 RNN 详细的原理就不多说了，还是比较简单的，可以直接参见李沐的教程

## 代码

写代码的时候，踩了不少的坑，在这里就做个总结吧

由于 RNN 具有状态（state），这造成了其结构和一般的神经网络还是有挺大区别的。

`torch.nn.RNN` 是 PyTorch 提供的一个用于循环神经网络（Recurrent Neural Network, RNN）的模块。它实现了最基本的 RNN 单元（非 LSTM 或 GRU），用于处理序列数据。

下面我们详细说明 `torch.nn.RNN` 的前向传播（forward propagation）过程。

---

### 一、输入与输出格式

```python
rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=False)
output, hn = rnn(input, h0)
```

#### 参数解释：

* **input\_size**：每个时间步输入的特征维度。
* **hidden\_size**：隐藏状态的维度。
* **num\_layers**：堆叠的 RNN 层数。
* **batch\_first**：若为 `True`，输入输出的 shape 为 `(batch, seq_len, input_size)`，否则为 `(seq_len, batch, input_size)`。

#### 输入张量：

* **input**：形状为 `(seq_len, batch, input_size)` 或 `(batch, seq_len, input_size)`。
* **h0**（可选）：初始隐藏状态，形状为 `(num_layers, batch, hidden_size)`。

#### 输出张量：

* **output**：包含每个时间步的输出，形状为 `(seq_len, batch, hidden_size)`，其实是由状态 ht 组成的张量。
* **hn**：最终时间步的隐藏状态，形状为 `(num_layers, batch, hidden_size)`。

所以如果我们是一个 **many to one** 的模型，在输出时只需要取 `output[-1]` 传递给全连接层即可；而如果是 **many to many** 就看情况取相应数量的状态。

---

### 二、单层 RNN 的前向传播原理

对于一个序列 $(x_1, x_2, ..., x_T)$，RNN 的每个时间步 $t$ 会执行如下操作：

#### 数学表达：

$$
h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})
$$

其中：

* $x_t$：当前时间步的输入，形状为 `(input_size,)`
* $h_{t-1}$：上一时间步的隐藏状态，形状为 `(hidden_size,)`
* $W_{ih}$：输入到隐藏层的权重矩阵，形状为 `(hidden_size, input_size)`
* $W_{hh}$：隐藏状态到隐藏层的权重矩阵，形状为 `(hidden_size, hidden_size)`
* $b_{ih}, b_{hh}$：偏置项，形状为 `(hidden_size,)`
* $\tanh$：激活函数

整个序列依次迭代执行该更新，产生每个时间步的隐藏状态 $h_t$。

---

### 三、状态初始化

在定义 RNN 类的时候，需要手动写一个状态初始化的方法，该方法返回一个维度为 `（num_layers, batch_size, hidden_size）` 的张量。实例代码如下：

```
class RNNRegression(nn.Module):
    def __init__(self, input_nums, hidden_nums, output_nums):
        super().__init__()
        self.rnn = nn.RNN(1, hidden_nums)
        self.linear = nn.Linear(hidden_nums, output_nums)
        self.hidden_nums = hidden_nums
        
    def forward(self, x, state):
        x = x.transpose(0, 1).unsqueeze(-1)  # (seq_len, batch_size, input_size) 4,32,1
        output, hn = self.rnn(x, state)
        y = self.linear(output[-1].reshape(-1, output.shape[-1]))
        return y.squeeze(), state
    
    def begin_state(self, device, batch_size):
        return torch.zeros((self.rnn.num_layers, batch_size, self.hidden_nums), device=device)
```

---

### 四、梯度裁剪

由于 RNN 的结构，过多的层会导致**梯度消失**或**梯度爆炸**，为了避免发生梯度爆炸，我们可以使用梯度裁剪的方法。

其具体操作也并不复杂，只需要加一行代码就可以了：`torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5.0, norm_type=2.0)`，`norm_type` 指定范数类型，默认是 L2 范数，其可以是任意的 p 范数。

> 当然除了裁剪之外，还有更加粗暴的方式——梯度截断，即直接把前面层的梯度舍弃掉。

---

### 五、预热

在 RNN 中，“**预热（warm-up）**” 是一个重要但容易被忽视的概念，尤其在处理长序列或状态传递模型时。它并不是 PyTorch 的内置功能，而是一种训练或推理时的策略。

RNN 的预热（warm-up） 指的是在开始对模型输出进行监督训练或评估之前，先输入一段“序列上下文”，仅用于初始化隐状态，但**不参与损失计算或梯度反传**。

- **为什么需要 warm-up？**

RNN 模型的预测依赖于其内部的**隐状态（hidden state）**。在没有足够上下文时，隐状态往往是随机的或者全零的，这样会导致模型在序列初始阶段预测不准确，尤其在以下场景中尤为明显：

1. 状态非常**依赖历史输入**（如轨迹预测、物理建模、金融时序）；
2. 训练使用**截断反向传播（truncated BPTT）**，模型看不到完整历史；
3. 使用非零初始状态会提升收敛速度或稳定性。

## 一些坑

这部分是我在写代码时遇到的一些问题。

最开始写好模型，进行训练时发现模型无法收敛，loss 很大，一直降不下去，后来检查了半天发现是损失函数输入的维度不匹配，两个输入的维度好像一个是 (32,1)，一个是 (32)，导致模型在计算损失的时候自动进行广播。

后来在一份文件中想要再写一个相似的模型处理不同的任务时，用了同一个优化器（optimizer），导致训练不收敛，也是 loss 下不去。后面检查了才发现问题所在。记住一个模型需要一个优化器，其和模型的参数是绑定的，不能用一个优化器优化多个模型的参数！（笨死了