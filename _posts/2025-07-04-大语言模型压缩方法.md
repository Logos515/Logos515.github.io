---
title: "大语言模型压缩方法"
layout: single
date: 2025-05-03
toc: true
toc_sticky: true  # 可选：目录是否固定在侧边
categories: [ "DeepLearning", "Quantization", "LLM"]
---

暑假正式开始了，论文的阅读计划也要提上日程。话说暑假真是忙碌，又要家教，又要练琴，又要科研的（）

> 昨晚忽然得知 nbuna 结婚了，有股恍如隔世之感。

<div align="center">
  <img src="/assets/images/2025/7.04/だから僕は音楽を辞めた.jpg" width="300">
  <figcaption>だから僕は音楽を辞めた — ヨルシカ</figcaption>
  <p></p>
</div>

<div align="center">
  <img src="/assets/images/2025/7.04/エルマ.jpg" width="300">
  <figcaption>エルマ — ヨルシカ</figcaption>
  <p></p>
</div>

## 前言

5 月初的一篇综述是 2022 年的，彼时大语言模型（LLM）刚开始发展，关于 LLM 的量化方法还不是很多，5202 年的现在，针对 LLM 的量化已经一批接着一批涌现出来了。为了与时俱进，我找了一篇 2024 年的综述，针对其中涉及到的量化方法进行简单的介绍。下图是论文中总结的关于 LLM 的量化方法大全，便于大家检索。

<div align="center">
  <img src="/assets/images/2025/7.04/ModelCompressionForLLM.png" width="600">
  <figcaption>图 1 针对 LLM 的模型压缩方法</figcaption>
  <p></p>
</div>

## 一、量化

不同量化方法的表现可以从下表中见到：

<div align="center">
  <img src="/assets/images/2025/7.04/Performance.png" width="600">
  <figcaption>表 1 不同量化方法的表现对比</figcaption>
  <p></p>
</div>

### （一）量化感知训练（QAT）

### （二）训练后量化（PTQ）

#### 1. 仅量化权重

##### (1) LUT-GEMM

是一种W4/A16的量化方法，突出贡献在于其在推理过程中不需要反量化。图 1 是其简略的框架图。

<div align="center">
  <img src="/assets/images/2025/7.04/LUT-GEMM/W4A16Quant.png" width="400">
  <figcaption>图 1 LUT-GEMM 框架图</figcaption>
  <p></p>
</div>

在介绍其原理之前，首先我们需要知道 LUT-GEMM 是基于 BCQ（Binary Coding Quantization）实现的，其是一种二值量化的方法，将权重向量表示成若干个经过缩放的二值向量的和：

$$ \omega = \sum_{i=1}^{q} \alpha_i \boldsymbol{b_i}, \quad \alpha_i \in \mathbb{R}^+, \quad \boldsymbol{b_i} \in \{-1,+1\}^n $$

简单来说，LUT 就是一张查询表（Look-up Table），根据输入的激活值进行计算，后续量化权重和输入的运算是通过查表的方式进行的。建表的原理我们通过下面的一个例子来说明。

我们假设有一个维度为 4 × 6 的 二值矩阵 $$B$$，输入 $$b$$ 是 6 × 1 的浮点数FP16，如下所示：

$$
B = \begin{bmatrix}
+1 & +1 & -1 & -1 & -1 & +1 \\
+1 & +1 & -1 & +1 & +1 & -1 \\
+1 & +1 & -1 & -1 & -1 & -1 \\
-1 & -1 & +1 & -1 & -1 & +1
\end{bmatrix}, \quad x = \begin{bmatrix}
x_1 & x_2 & x_3 & x_4 & x_5 & x_6
\end{bmatrix}.
$$

如果我们直接计算 $$B$$ 和 $$x$$ 的乘积，会发现**存在大量的冗余计算**，例如 $$x_1+x_2-x_3$$ 被重复计算了多次，这样的计算冗余会随着矩阵 B 维度的增加而增加，因此我们可以通过建表的方式来减少运算量。

具体来说，我们可以对 $$x$$ 的分量进行分组，假设 $$\mu$$ 个分量为一组，那么这几个分量之间的所有可能结果一共有 $$2^\mu$$ 种，我们将其提前计算，并以表格的形式存储起来，等到要用的时候再进行查询即可。这一过程的时间复杂度约为 $$\mathcal{O}\big(m \cdot \frac{n}{\mu} \cdot q \big)$$

为了能够进一步加速，作者在 GPU 上实现时还将其以线程为粒度进行了优化，如下图 2 所示。

<div align="center">
  <img src="/assets/images/2025/7.04/LUT-GEMM/OverViewLUT-GEMM.png" width="700">
  <figcaption>图 2 LUT-GEMM 在 GPU 上的实现</figcaption>
  <p></p>
</div>

这里我们把矩阵 $$B$$ 分成了九块，交给九个线程块（Thread Block）分别处理，假设一个线程块包含四个线程，一个 TB 中的四个线程共享四张 LUT，这里我们的 $$\mu=8$$，即一张 LUT 由 256 项构成。这张图还是挺清晰的，整体的思想倒是并不难理解。

除此之外，为了确保量化精度，作者采用了分组量化的方法，将输入的维度进行分组，每一组共用一个缩放系数。实验证明一组的数量越多，延迟越小（矩阵乘法完成时间越短），这是由于单批量操作 (single-batch operations) 主要受限于内存 (memory-bound)，并且延迟与内存占用成正比。

作者还在原本的 BCQ 的基础上加上了一个偏置项：

$$ \omega = \sum_{i=1}^{q} \alpha_i \boldsymbol{b_i} + z $$

这使得 BCQ 可以表示零点，成为一种非对称量化方法，同时作者还从理论上证明了经过合理的取值，BCQ 可以成为均匀量化方法。（这也是这篇文章的一个重要贡献）

#### 2. 权重和激活值量化

#### 3. KV 缓存量化

## 二、剪枝

### （一）非结构化剪枝

### （二）结构化剪枝

### （三）半结构化剪枝

## 三、知识蒸馏

### （一）黑盒蒸馏

#### 1. 思维链（COT）

#### 2. 上下文学期（Comtext Learning）

#### 3. 指令遵循（Instruction Following）

### （二）白盒蒸馏

## 四、低秩分解