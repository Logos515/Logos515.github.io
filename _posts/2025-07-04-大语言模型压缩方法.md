---
title: "大语言模型压缩方法"
layout: single
date: 2025-07-04
toc: true
toc_sticky: true  # 可选：目录是否固定在侧边
categories: [ "DeepLearning", "Quantization", "LLM"]
---

暑假正式开始了，论文的阅读计划也要提上日程。话说暑假真是忙碌，又要家教，又要练琴，又要科研的（）

> 昨晚忽然得知 nbuna 结婚了，有股恍如隔世之感。

<div align="center">
  <img src="/assets/images/2025/7.04/だから僕は音楽を辞めた.jpg" width="300">
  <figcaption>だから僕は音楽を辞めた — ヨルシカ</figcaption>
  <p></p>
</div>

<div align="center">
  <img src="/assets/images/2025/7.04/エルマ.jpg" width="300">
  <figcaption>エルマ — ヨルシカ</figcaption>
  <p></p>
</div>

## 前言

5 月初阅读的一篇综述是 2022 年的，彼时大语言模型（LLM）刚开始发展，关于 LLM 的量化方法还不是很多，5202 年的现在，针对 LLM 的量化已经一批接着一批涌现出来了。为了与时俱进，我找了一篇 2024 年的综述，针对其中涉及到的量化方法进行简单的介绍。下图是论文中总结的关于 LLM 的量化方法大全，便于大家检索。

<div align="center">
  <img src="/assets/images/2025/7.04/ModelCompressionForLLM.png" width="600">
  <figcaption>图 1 针对 LLM 的模型压缩方法</figcaption>
  <p></p>
</div>

## 一、量化

不同量化方法的表现可以从下表中见到：

<div align="center">
  <img src="/assets/images/2025/7.04/Performance.png" width="600">
  <figcaption>表 1 不同量化方法的表现对比</figcaption>
  <p></p>
</div>

### （一）量化感知训练（QAT）

### （二）训练后量化（PTQ）

#### 1. 仅量化权重

##### (1) LUT-GEMM

论文链接：https://arxiv.org/abs/2206.09557，引用次数：162，发表于：ICLR 2024

是一种W4/A16的量化方法，突出贡献在于其在推理过程中不需要反量化。图 1 是其简略的框架图。

<div align="center">
  <img src="/assets/images/2025/7.04/LUT-GEMM/W4A16Quant.png" width="400">
  <figcaption>图 1 LUT-GEMM 框架图</figcaption>
  <p></p>
</div>

在介绍其原理之前，首先我们需要知道 LUT-GEMM 是基于 BCQ（Binary Coding Quantization）实现的，其是一种二值量化的方法，将权重向量表示成若干个经过缩放的二值向量的和：

$$ \omega = \sum_{i=1}^{q} \alpha_i \boldsymbol{b_i}, \quad \alpha_i \in \mathbb{R}^+, \quad \boldsymbol{b_i} \in \{-1,+1\}^n $$

简单来说，LUT 就是一张查询表（Look-up Table），根据输入的激活值进行计算，后续量化权重和输入的运算是通过查表的方式进行的。建表的原理我们通过下面的一个例子来说明。

我们假设有一个维度为 4 × 6 的 二值矩阵 $$B$$，输入 $$b$$ 是 6 × 1 的浮点数FP16，如下所示：

$$
B = \begin{bmatrix}
+1 & +1 & -1 & -1 & -1 & +1 \\
+1 & +1 & -1 & +1 & +1 & -1 \\
+1 & +1 & -1 & -1 & -1 & -1 \\
-1 & -1 & +1 & -1 & -1 & +1
\end{bmatrix}, \quad x = \begin{bmatrix}
x_1 & x_2 & x_3 & x_4 & x_5 & x_6
\end{bmatrix}.
$$

如果我们直接计算 $$B$$ 和 $$x$$ 的乘积，会发现**存在大量的冗余计算**，例如 $$x_1+x_2-x_3$$ 被重复计算了多次，这样的计算冗余会随着矩阵 B 维度的增加而增加，因此我们可以通过建表的方式来减少运算量。

具体来说，我们可以对 $$x$$ 的分量进行分组，假设 $$\mu$$ 个分量为一组，那么这几个分量之间的所有可能结果一共有 $$2^\mu$$ 种，我们将其提前计算，并以表格的形式存储起来，等到要用的时候再进行查询即可。这一过程的时间复杂度约为 $$\mathcal{O}\big(m \cdot \frac{n}{\mu} \cdot q \big)$$

为了能够进一步加速，作者在 GPU 上实现时还将其以线程为粒度进行了优化，如下图 2 所示。

<div align="center">
  <img src="/assets/images/2025/7.04/LUT-GEMM/OverViewLUT-GEMM.png" width="700">
  <figcaption>图 2 LUT-GEMM 在 GPU 上的实现</figcaption>
  <p></p>
</div>

这里我们把矩阵 $$B$$ 分成了九块，交给九个线程块（Thread Block）分别处理，假设一个线程块包含四个线程，一个 TB 中的四个线程共享四张 LUT，这里我们的 $$\mu=8$$，即一张 LUT 由 256 项构成。这张图还是挺清晰的，整体的思想倒是并不难理解。

除此之外，为了确保量化精度，作者采用了分组量化的方法，将输入的维度进行分组，每一组共用一个缩放系数。实验证明一组的数量越多，延迟越小（矩阵乘法完成时间越短），这是由于单批量操作 (single-batch operations) 主要受限于内存 (memory-bound)，并且延迟与内存占用成正比。

作者还在原本的 BCQ 的基础上加上了一个偏置项：

$$ \omega = \sum_{i=1}^{q} \alpha_i \boldsymbol{b_i} + z $$

这使得 BCQ 可以表示零点，成为一种非对称量化方法，同时作者还从理论上证明了经过合理的取值，BCQ 可以成为均匀量化方法。（这也是这篇文章的一个重要贡献）

##### (2) GPTQ

论文链接：https://arxiv.org/abs/2210.17323，引用次数：1162，发表于：ICLR 2023

是一种针对大模型的量化方法，其特点就是：**适用于大模型、需要的算力少、量化模型精度损失小**。最突出的贡献就是 GPTQ 是第一个能够将大模型量化到 3-4bit 的方法，其证明了对 LLM 进行量化的可行性。

GPTQ 的思想来源于 Yann LeCun 在 1990 年提出的 OBD 算法。OBD、OBS、OBC、OBQ 是一系列剪枝和量化算法，我们一一对其进行介绍。

- **OBD 和 OBS**

OBD 是一种剪枝算法，其思想很简单，就是希望剪枝的结果对目标函数造成的影响最小。我们对目标函数进行泰勒展开可以得到：

$$\Delta E = \sum_i g_i \Delta w_i + \frac{1}{2} \sum_i h_{ii} (\Delta w_i)^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \Delta w_i \Delta w_j + O(\Delta w^3)$$

其中 g 是偏导，h 是海森矩阵，OBD 假设模型经过训练之后，其关于各维度的一阶偏导应该是 0，所以第一项直接省略，如果我们再对高阶项进行省略就得到下面的式子：

$$\Delta E = \frac{1}{2} \sum_i h_{ii} (\Delta w_i)^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \Delta w_i \Delta w_j$$

OBD 算法直接忽略了交叉项，其假设每个参数对目标函数的影响是**独立的**。所以又把上面的第二项忽略了。

而 OBS 则没有忽略交叉项，其将剪枝问题建模成一个约束优化问题，如下：

$$\min_{\Delta \mathbf{w}, q} \frac{1}{2} \Delta \mathbf{w}^\text{T} \mathbf{H} \Delta \mathbf{w} \quad s.t. \quad \mathbf{e}_q^\text{T} \cdot \Delta \mathbf{w} + w_q = 0$$

使用拉格朗日法求解得到结果：

$$\Delta \mathbf{w} = -\frac{w_q}{[\mathbf{H}^{-1}]_{qq}} \mathbf{H}^{-1} \cdot \mathbf{e}_q \quad \text{and} \quad L = \frac{1}{2} \frac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}$$

在剪枝的时候，每次寻找对最终目标函数影响最小的权重，设为 0，并更新其他权重，如此迭代即可。

- **OBC 和 OBQ**

上面的算法的问题是**计算量比较大**，我们需要求解海森矩阵，当模型的参数量和输入维度很大的时候，求解海森矩阵花费的时间太长。所以 OBC 采取的措施是**假设参数矩阵的同一行参数互相之间是相关的**，而**不同行之间的参数互不相关**，这样，海森矩阵就只需要在每一行内单独计算就行啦。

其实现原理的示意图如下所示：

<div align="center">
  <img src="/assets/images/2025/7.04/GPTQ/OBS.png" width="400">
  <figcaption>图 1 OBS 原理示意图</figcaption>
  <p></p>
</div>

<div align="center">
  <img src="/assets/images/2025/7.04/GPTQ/OBQ.png" width="400">
  <figcaption>图 2 OBQ 原理示意图</figcaption>
  <p></p>
</div>

整个 OBC 的算法流程大致为：

1. 对一行的权重求海森矩阵
2. 按照顺序对该行的k个权重进行剪枝，并更新其他参数
3. 删除海森矩阵的 p 行 p 列，再求逆

OBQ 是一种量化的方法，其**将量化视为特殊的剪枝**，只是把 OBC 的约束条件进行了调整：

$$\mathbf{e}_q^\text{T} \cdot \Delta \mathbf{w} + w_q = quant(w_q)$$

同时把 OBC 推导公式中的 $$w_q$$ 换成 $$w_q - quant(w_q)$$，就得到了 OBQ 量化的参数更新公式：

$$\Delta \mathbf{w} = -\frac{w_q - quant(w_q)}{[\mathbf{H}^{-1}]_{qq}} \mathbf{H}^{-1} \cdot \mathbf{e}_q \quad \text{and} \quad L = \frac{1}{2} \frac{(w_q - quant(w_q))^2}{[\mathbf{H}^{-1}]_{qq}}$$

整个 OBQ 算法的伪代码如下图所示：

<div align="center">
  <img src="/assets/images/2025/7.04/GPTQ/OBQ_algorithm.png" width="400">
  <figcaption>图 3 OBQ 算法伪代码</figcaption>
  <p></p>
</div>

- GPTQ

GPTQ 算法在前人的基础上做了一些改进。

首先是其发现当模型的参数量比较大的时候，**权重量化的先后顺序带来的影响很小**，所以我们干脆直接按照索引从第一个开始逐个往后量化。

其次，其假设权重矩阵的不同行权重的海森矩阵是一样的，这是因为不同行的权重处理的都是相同的输入，海森矩阵完全由未量化参数对应的输入决定。这样就可以并行地处理每一行了，大大提高了量化的速度。

为了能够**缓解 IO 造成的速度瓶颈**，GPTQ 将权重矩阵按列进行分组形成一个个 Block，每次量化一个 Block 的权重，其量化的规则是：当量化到某一列时，实时更新对应 Block 中的权重，而对于不在同一个 Block 且未量化的权重，先保留改变量，等一整个 Block 处理完之后再进行参数更新。

此外，GPTQ 使用 **Cholesky 分解**求海森矩阵的逆，在增强数值稳定性的同时，不再需要对海森矩阵做更新计算，进一步减少了计算量。（这一部分不是很懂）

整个 GPTQ 的算法流程如下：

<div align="center">
  <img src="/assets/images/2025/7.04/GPTQ/GPTQ_algorithm.png" width="400">
  <figcaption>图 4 GPTQ 算法伪代码</figcaption>
  <p></p>
</div>

> 这里海森矩阵的逆的计算值得注意，给对角线加入了一个小值 $$\lambda$$ 可以减少数值计算带来的误差。（虽然不太清楚其中原理。

作者在 OPT 和 BLOOM 模型上进行了实验，结果如下图所示：

<div align="center">
  <img src="/assets/images/2025/7.04/GPTQ/results.png" width="400">
  <figcaption>图 5 GPTQ 实验结果图</figcaption>
  <p></p>
</div>

可以看到相比于 RTN 方法，GPTQ 的量化精度更高，并且更加稳定。

#### 2. 权重和激活值量化

#### 3. KV 缓存量化

## 二、剪枝

### （一）非结构化剪枝

### （二）结构化剪枝

### （三）半结构化剪枝

## 三、知识蒸馏

### （一）黑盒蒸馏

#### 1. 思维链（COT）

#### 2. 上下文学期（Comtext Learning）

#### 3. 指令遵循（Instruction Following）

### （二）白盒蒸馏

## 四、低秩分解