---
title: "从 RNN 到 Transformer"
layout: single
date: 2025-05-20
toc: true
toc_sticky: true  # 可选：目录是否固定在侧边
categories: [ "Python",]
tags: ["Survey", "DeepLearning"]
---

这篇博客是我在学习了李沐大神的《动手学深度学习（中文版）》的第八章循环神经网络，第九章现代循环神经网络，第十章注意力机制之后写就的，预期会结合自己对 RNN 和 Attention 的理解讨论神经网络是如何从 RNN 发展到 Transformer 的。

<div align="center">
  <img src="/assets/images/5.20/月を歩いている.jpg" width="300">
  <figcaption>月を歩いている — n-buna</figcaption>
  <p></p>
</div>

## RNN的出现

### 自回归模型

RNN 之前，我们学习了多层感知机和卷积神经网络，而这两种模型之所以有效，是因为它们所处理的数据都是**独立同分布**的。而现实中经常会遇到一些数据不是独立同分布的，而是存在**时间或空间上的顺序**，我们称之为**序列数据**（例如文章中的文字、视频的图像帧）。打乱序列数据的顺序会显著改变数据原有的信息，使其丧失原本的意义。为了能够对序列模型进行处理，我们希望能够建立一个模型，其能够捕捉到序列中含有的顺序信息。

这样的模型显然可以很好地完成时序预测的任务，即给定前 t-1 时刻的数据，预测 t 时刻的数据。

- **自回归模型**

自回归模型是一个直接的例子，它直接把前 $$\tau$$ 时刻的数据作为输入，直接预测下一个时刻的输出：

$$x_t \sim P(x_t|x_{t-1}, x_{t-2}, x_{t-3}, ..., x_{t-\tau}) \tag{1}$$

- **隐变量自回归模型**

另一种模型则更加巧妙，其通过保留一些对过去观测的总结 $$h$$，同时更新预测和新的总结：

$$
h_t = g(h_{t-1}, x_{t-1})  \\
\hat x_t = P(x_t | h_t)
$$

其大体的框架如下图所示：

<div align="center">
  <img src="/assets/images/5.20/sequence-model.svg" width="400">
  <figcaption>图 1 隐变量自回归模型</figcaption>
  <p></p>
</div>

这两种模型是如何生成训练数据的呢？一个经典方法是使用历史观测来预测下一个未来预测。由于时间会不断改变，一个常见的假设是虽然特定值 $$x_t$$ 可能会改变，但是**序列本身的动力学**不会改变。这样的假设是合理的，因为新的动力学一定受新的数据影响，而我们不可能用目前所掌握的数据来预测新的动力学。统计学家称不变的动力学为静止的（stationary）。因此，整个序列的估计值都将通过下面的方式获得：

$$P(x_1,...,x_T) = \prod\limits_{t=1}^{T}P(x_t|x_{t-1},...,x_1) \tag{2}$$

> 注意，如果是处理离散的对象，就需要转化为分类问题

### 马尔可夫模型

马尔可夫模型太常见了，这里就简单讲讲它的概念。

马尔可夫模型是符合如下假设的模型：t 时刻模型的状态仅仅取决于其前 $$\tau$$ 个时刻的状态，与更前面的状态无关。特殊情况，当 $$\tau = 1$$ 的时候，我们称之为一阶马尔可夫模型。

> 类似地，我们可以定义二阶、三阶马尔可夫模型

### 因果关系

往往时序模型是带有因果关系的，虽然有时我们的模型可以求出反向的条件概率分布，但这**并不意味着其是有意义的**。如果我们改变 $$x_t$$，显然 t 时刻之前的值、分布是不受到影响的。

### 自回归模型代码

实验部分，我们使用了一个简单的两层的神经网络进行预测，代码请见官方教程。结果显示该模型在短期的预测上还是很准确的，但是**一旦步数增加之后（从 16-step 开始），就开始出现较大偏差**。

另外，如果直接使用预测的值作为输入来进行多步预测的话，会导致**误差不断积累**，预测的效果奇差无比。

> 内插法：在现有观测值之间进行估计
> 外推法：对超出已知观测范围进行预测

### 循环神经网络

上面我们讲的一个简单的自回归模型其实就是一个没有隐状态的神经网络，或者说其隐状态就是全连接层第一层的输出，其不是一个很好的表征。

所以我们下面考虑有隐状态的循环神经网络：

<div align="center">
  <img src="/assets/images/5.20/rnn-train.svg" width="500">
  <figcaption>图 2 基于循环神经网络的字符级语言模型</figcaption>
  <p></p>
</div>

还是比较简单的，其原理就用下面的两个公式解释了：

$$
H_t = \phi(X_t W_{xh} + H_{t-1} W_{hh} + b_h) \\
O_t = H_tW_{hq} + b_q
$$

在评估模型时，我们使用 **困惑度（Perplexity）** 作为指标，其定义如下：

$$\text{exp}\big(-\frac{1}{n}\sum\limits_{t=1}^{n}\log P(x_t|x_{t-1},...,x_1)\big) \tag{3}$$

在最好的情况下困惑度为1，最坏的情况下困惑度为正无穷大，基线是随机预测，困惑度为词表中唯一词元的数量。

代码部分的话，就讲一下 `torch.nn.RNN` 的用法吧：

定义一个循环神经网络层的代码为 `rnn = torch.nn.RNN(input_size, hidden_size, num_layers)`

其中各个参数的含义如下表所示：

| 参数名             | 说明                                                            |
| --------------- | ------------------------------------------------------------- |
| `input_size`    | 每个时间步输入向量的维度                                                  |
| `hidden_size`   | 隐状态（hidden state）的维度                                          |
| `num_layers`    | RNN 堆叠的层数（默认为 1）                                              |
| `nonlinearity`  | 激活函数，默认 `'tanh'`，可设为 `'relu'`                                 |
| `batch_first`   | 若为 True，输入输出为 (batch, seq, feature)，否则为 (seq, batch, feature) |
| `dropout`       | 多层时使用 dropout（只在层与层之间）                                        |
| `bidirectional` | 是否使用双向 RNN                                                    |

- **前向传播过程**

如果 `batch_first=True`，则：

输入：`X` 的维度是 `(batch_size, seq_len, input_size)`

输出有两个：`output`：所有状态组成的张量，维度为 `(batch_size, seq_len, hidden_size * num_directions)`；`hn`：最后一个时间步的隐状态，维度为 `(num_layers * num_directions, batch_size, hidden_size)`

一般我们会选取 `output[-1]` 取出最后一个输出的状态，用于后续操作。

> 如果 `batch_size=False` 那么需要交换输入和输入的前两个维度

- **反向传播过程**

梯度的公式就不推导了，根据模型的结构图应该很容易就能推导出来，值得注意的是我们需要递归的去计算梯度 $$\partial h_t / \partial w_h$$ 的值，公式如下：

$$\frac{\partial h_t}{\partial w_h} = \frac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h} + \sum\limits_{i=1}^{t-1}\bigg(\prod\limits_{j=i+1}^{t}\frac{\partial f(x_j,h_{j-1},w_h)}{\partial h_{j-1}}\bigg)\frac{\partial f(x_i,h_{i-1},w_h)}{\partial w_h} \tag{4}$$

当 t 很大时，这个链会变得很长，难以计算，如果进行完全计算的话会导致梯度爆炸或梯度消失，为了解决这一问题，我们可以采取**截断时间步**的策略。

标准的 Backpropagation Through Time（BPTT）采用的固定步数截断梯度。

我们不对完整的时间序列做反向传播，而是：每训练几步，就只在这几步时间内计算和目标的损失，传播梯度，剪掉更早时间步的依赖（使用detach()方法），这样可以减小计算图的大小，防止长距离依赖造成的梯度问题。

随机截断就是在标准的 BPTT 上，采用随机步长进行截断的方法。

- **Tips**

截断 BPTT 会损失部分长期依赖信息，但换来**更快训练、稳定性更高**；对于很长序列（如文本生成、时间序列预测），是训练 RNN 的标准做法；对于 nn.LSTM 和 nn.GRU 也一样适用。

- 几种方法的比较

<div align="center">
  <img src="/assets/images/5.20/truncated-bptt.svg" width="500">
  <figcaption>图 3 比较RNN中计算梯度的策略，3行自上而下分别为：随机截断、常规截断、完整计算</figcaption>
  <p></p>
</div>

图 3 说明了当基于循环神经网络使用通过时间反向传播分析《时间机器》书中前几个字符的三种策略：

第一行采用随机截断，方法是将文本划分为不同长度的片断；

第二行采用常规截断，方法是将文本分解为相同长度的子序列。 这也是我们在循环神经网络实验中一直在做的；

第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。

遗憾的是，虽然随机截断在理论上具有吸引力， 但很可能是由于多种因素在实践中并不比常规截断更好。 首先，在对过去若干个时间步经过反向传播后， 观测结果**足以捕获实际的依赖关系**。 其次，**增加的方差抵消了时间步数越多梯度越精确的事实**。 第三，我们真正想要的是只有**短范围交互**的模型。 因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。

简单来说，就是使用固定步长截断就够了，因为大多数模型只需要捕捉局部的信息就足矣，随机截断反而弄巧成拙了。

另外，由于状态在传递过程中始终和固定的矩阵 $$W_{hh}$$ 相乘，所以在求梯度的时候会遇到**矩阵的高次幂**，而在这个幂中，**小于1的特征值将会消失，大于1的特征值将会发散**。这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。