这篇文章是我在读了一篇模型量化的综述后动笔写就的，本人水平极其有限，写文章只是为了记录学习，如果有任何错误请及时指出并联系我修改！

## **摘要**

## **背景**

自从 **ChatGPT** 发布以来，AI 浪潮席卷全球，而 AI 圈内则是各种大模型盛行。**Scaling Law** 的成立使得每一家企业都在训练越来越大的神经网络模型，但是这些大模型具有显著的缺点——**笨重**。即大模型需要大量的算力进行训练和推理，并且需要占据大量的存储空间，这意味着对于充斥着边缘设备的工业界来说，大模型的部署需要极大的成本。而边缘设备本身能够提供的资源是相当有限的，将无比巨大的模型直接部署到边缘设备上一是成本高，二是时效性差，无法满足工业界的需求。所以我们需要使用特殊的方法来提升模型的效率，能够在尽量不降低模型性能的情况下减少模型的参数，加快模型的训练和推理（主要是推理）。在这样的需求下，模型量化的方法自然成为一种主流。

量化在数字信号处理领域是指将信号的连续取值（或者大量可能的离散取值）近似为有限多个（或较少的）离散值的过程。神经网络模型的量化也是类似的，通过建立一个映射，将原来的需要更多比特表示的参数使用更少的比特表示。例如原来的神经网络的参数是用 32bit 表示的，我们可以建立映射，用 8bit 来表示每一个参数，这样就能够直接减少 4 倍的存储，推理的速度也更快了。

从上面的描述中我们也能看出，量化的最终目的可以概括为两个：

1. 尽可能保持模型原来的性能
2. 尽可能减少模型的“重量”，加快模型的训练或推理

## 方法

根据量化方法的不同特点，其可以分成以下几种方式：

### 均匀/非均匀量化

所谓**均匀量化（Uniform quantization）**就是指有**固定的量化精度**的方法，例如最常见的整数量化：

$$Q(r)=\text{Int}(r/S)-Z$$

在这个式子中，$$Q$$ 是量化器，$$r$$ 是浮点输入，$$S$$ 是缩放因子，$$Z$$ 是整数零点（虽然是零点，但不一定是 0 哦！此处的“零点”是指当输入为 0 时对应的量化值），而 Int 指的是四舍五入方法，将一个实数映射为整数。整数量化拥有固定的量化精度——1，所以它是一种均匀量化方法。

<div align="center">
  <img src="../assets/images/模型量化简介/uniform non-uniform quantization.png" width="600">
  <figcaption>图 1：均匀与非均匀量化示意图</figcaption>
  <p></p>
</div>

而非均匀量化则是相反的，其没有一个固定的量化精度，即其量化值并不是均匀的。均匀量化和非均匀量化的对比示意图可以从图 1 中看出。

- **反量化**

在这里我们引入反量化的概念，顾名思义，反量化就是**将量化值还原为原始值**的过程。由于量化会造成数据精度的损失，所以反量化并不能完全还原原始的数据。我们上面提到的整数量化，其反量化可以通过下面的公式实现：

$$\tilde r=S(Q(r)+Z)$$

### 对称/非对称量化

