这篇文章是我在读了一篇模型量化的综述后动笔写就的，本人水平极其有限，写文章只是为了记录学习，如果有任何错误请及时指出并联系我修改！

## **摘要**

## **背景**

自从 **ChatGPT** 发布以来，AI 浪潮席卷全球，而 AI 圈内则是各种大模型盛行。**Scaling Law** 的成立使得每一家企业都在训练越来越大的神经网络模型，但是这些大模型具有显著的缺点——**笨重**。即大模型需要大量的算力进行训练和推理，并且需要占据大量的存储空间，这意味着对于充斥着边缘设备的工业界来说，大模型的部署需要极大的成本。而边缘设备本身能够提供的资源是相当有限的，将无比巨大的模型直接部署到边缘设备上一是成本高，二是时效性差，无法满足工业界的需求。所以我们需要使用特殊的方法来提升模型的效率，能够在尽量不降低模型性能的情况下减少模型的参数，加快模型的训练和推理（主要是推理）。在这样的需求下，模型量化的方法自然成为一种主流。

量化在数字信号处理领域是指将信号的连续取值（或者大量可能的离散取值）近似为有限多个（或较少的）离散值的过程。神经网络模型的量化也是类似的，通过建立一个映射，将原来的需要更多比特表示的参数使用更少的比特表示。例如原来的神经网络的参数是用 32bit 表示的，我们可以建立映射，用 8bit 来表示每一个参数，这样就能够直接减少 4 倍的存储，推理的速度也更快了。

从上面的描述中我们也能看出，量化的最终目的可以概括为两个：

1. 尽可能保持模型原来的性能
2. 尽可能减少模型的“重量”，加快模型的训练或推理

## 方法

根据量化方法的不同特点，其可以分成以下几种方式：

### 均匀/非均匀量化

所谓**均匀量化（Uniform quantization）**就是指有**固定的量化精度**的方法，例如最常见的整数量化：

$$Q(r)=\text{Int}(r/S)-Z \tag{1} $$

在这个式子中，$$Q$$ 是量化器，$$r$$ 是浮点输入，$$S$$ 是缩放因子，$$Z$$ 是整数零点（虽然是零点，但不一定是 0 哦！此处的“零点”是指当输入为 0 时对应的量化值），而 Int 指的是四舍五入方法，将一个实数映射为整数。整数量化拥有固定的量化精度——1，所以它是一种均匀量化方法。

<div align="center">
  <img src="../assets/images/模型量化简介/uniform non-uniform quantization.png" width="600">
  <figcaption>图 1：均匀与非均匀量化示意图</figcaption>
  <p></p>
</div>

而非均匀量化则是相反的，其没有一个固定的量化精度，即其量化值并不是均匀的。均匀量化和非均匀量化的对比示意图可以从图 1 中看出。

非均匀量化常被用来处理钟形分布和长尾分布的参数，常见的非均匀量化规则例如采用对数分布使量化步长成指数增加，或者使用二值向量的线性组合来表示原向量等。最近的工作把模型量化定义为一个优化问题：

$$ \min_Q |Q(r)-r|^2 \tag{2} $$

由于非均匀量化方法难以在通用的计算硬件（GPU 和 CPU）上高效部署，业内普遍采用的还是均匀量化方法。

- **反量化**

在这里我们引入反量化的概念，顾名思义，反量化就是**将量化值还原为原始值**的过程。由于量化会造成数据精度的损失，所以反量化并不能完全还原原始的数据。我们上面提到的整数量化，其反量化可以通过下面的公式实现：

$$\tilde r=S(Q(r)+Z) \tag{3} $$

### 对称/非对称量化

还记得我们在均匀量化中提到的那个公式（1）吗？

这个公式中的 $$S$$ 被称作缩放因子，其可以由下面的式子计算得到：

$$S=\frac{\beta-\alpha}{2^b-1} \tag{4}$$

公式中的 $$\beta$$ 和 $$\alpha$$ 确定了一个量**化的范围**，例如 $$\beta=32, \alpha=-32, b=4$$ 意味着我们把 [-32, 32] 范围内的值使用 4bit 表示，代入计算得到 $$S=4.27$$，这意味着原值每增加 4.27，量化后的值就要增加 1，所以从这个例子也能看出这里的 $$S$$ 所表示的含义就是：**当我们使用 b 个 bit 位对原值进行量化操作时，原值每增加 $$S$$， 量化后的值就需要增加 1**

根据量化范围（$$\alpha 和 \beta$$）的特点，我们可以把量化方法分为**对称量化**和**非对称量化**。

- 当 $$\alpha = \beta$$ 时，是对称量化
- 当 $$\alpha \ne \beta$$ 时，是非对称量化

可以从下面的图 2 中直观地看出两者的不同：

<div align="center">
  <img src="../assets/images/模型量化简介/symmetric and asymmetric quantization.png" width="800">
  <figcaption>图 2：对称与非对称量化对比图</figcaption>
  <p></p>
</div>

相比而言，对称量化更加的直观简便，只需要确定一个对称的量化范围，然后让公式（1）中的 $$Z=0$$ 即可。常见的对称量化方法是直接**取最大/最小值的绝对值中较大**的作为量化范围的一半或者根据数据分布手动确定范围，然后使用 clip 方法裁剪溢出的值（例如当量化范围为 [-3, 3] 时，值 3.5 直接被放缩到 3）。当然，这种方法过于简单，容易受到异常值的影响导致量化精度的下降，所以我们也可以采用**分位数**来代替最值，或者使用更加高级的方法（比如最小化原始数据分布和量化数据分布的**KL散度**等）

而非对称量化则具有**更加紧密的裁剪范围**，因为其可以根据实际的数据分布设计裁剪范围，从而实现更好的量化效果。尤其是在非线性层为 **ReLU** 函数的情况下，如果我们采用对称量化，会导致有一半的量化范围被浪费了，而采用非对称量化能够使精度提升整整一倍！

一般对称量化常用于量化**模型的权重**，由于 $$Z=0$$， 所以在进行矩阵乘法运算时可以**直接忽略大量的交叉项运算**。
而非对称量化则常用于量化**激活值**，因为输入的数据分布通常不是对称的，比如图片输入是 0-255 的像素值，如果使用对称量化会导致模型性能大幅降低。而**非对称量化所带来的交叉项的计算则可以被吸收到 bias 中**（这里挖个坑，后面有时间填），所以其相比于对称量化基本不会增加太多推理时间。

### 动态/静态量化

选择量化范围（$$\alpha, \beta$$）的过程称之为**校准**，目前我们已经讨论了很多校准的方法，但它们都不会在模型推理的过程中发生改变，然而对于不同的输入其激活图是不同的，我们还可以把量化这件事做得更加精细。所以就有了静态/动态量化的方法。

- **动态量化**：对每一个输入都计算其**专属的量化范围**，这意味着我们需要在运行过程中实时计算数据的特征（最值、均值等），然后利用其特征确定量化范围。虽然这一过程会带来计算量的增加，但其能够有效提升模型的精度。
- **静态量化**：所谓静态量化就是针对所有的输入都使用**同一个量化范围**，这意味着在推理时量化范围是确定的，我们需要使用初始数据的特征计算量化范围，然后在整个推理过程中应用。目前有许多方法可以找到最优的量化范围，比如最小化模型权重量化前后的 MSE 或 CE；另外还有很多方法是将量化的范围作为参数，在训练阶段和 NN 权重一起学习的，这一类量化器我们称之为可学习量化器。

> 目前相关行业从业者普遍采用的是静态量化方法。

### 量化粒度

量化粒度这一概念是针对卷积神经网络提出的，实际上其和校准类似，根据模型的权重来计算量化范围的。由于一个 CNN 模型中通常含有许多卷积层，每一层卷积层又有多个滤波器，根据不同的精度需求，我们可以有多种确定量化范围的方式。我们可以列出以下几种量化粒度：
1. **分层量化**：每一个卷积层使用相同的量化范围，不同的卷积层之间采用不同的量化范围。
2. **分组量化**：每一个卷积层的多个滤波器（卷积核）进行分组，每一组具有相同的量化范围。
3. **通道量化**：每一个卷积层的每一个滤波器之间都采用不同的量化范围。
4. **子通道量化**：同一个滤波器内部采用不同的量化范围（例如按行/列量化）。

不同量化粒度之间的差异可以从图 3 中看出：

<div align="center">
  <img src="../assets/images/模型量化简介/quantization granularity.png" width="800">
  <figcaption>图 2：分层量化与通道量化对比图</figcaption>
  <p></p>
</div>

图 3 中，可以看到 Layer 1 具有多个滤波器，分层量化对所有的滤波器都采用相同的量化范围，而通道量化是根据每一个滤波器的参数来确定量化范围的。

从前面的讨论中，不难看出不同的量化粒度，其计算成本和量化效果的差异是不同的。越是细粒度（子通道量化）的方法，其量化效果越好，但是计算的成本也越高。考虑到准确率和计算效率的平衡，业内普遍采用通道量化方法来量化卷积核。

