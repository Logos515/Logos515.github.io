---
title: "神经网络优化方法"
layout: single
date: 2025-06-04
toc: true
toc_sticky: true  # 可选：目录是否固定在侧边
categories: [ "Python" ]
tags: ["DeepLearning"]
---

本篇文章主要记录自己在学习深度学习时看到的一些关于神经网络优化知识，当前主要参考的是李沐《动手学深度学习》第十一章的内容，感兴趣的同学可以自行阅读教程。本博客会对教程中的一些细节进行深入探讨，例如重参数化等。

<div align="center">
  <img src="/assets/images/2025/6.04/カーテンコールが止む前に.jpg" width="300">
  <figcaption>カーテンコールが止む前に — n-buna</figcaption>
  <p></p>
</div>

### 训练风险与实际风险

关于训练风险和实际风险的区别，在大三上学期开设的《机器学习》课程中就讲过了，在西瓜书中有更详细的介绍。其主要思想并不难理解，就是损失函数的最小值点不代表将模型泛化到所有数据上时都是最小值。

训练风险和实际风险之间的关系可以用一个不等式衡量，该公式似乎又和 VC 维有关。感兴趣的同学请看西瓜书《计算学习理论》一章节的内容，内容侧重于理论分析，对于我这种数学不好的人来说难度比较大（），所以我对自己的要求是只要记住结论就行。

### 优化挑战

#### 局部极小值与鞍点

神经网络在求解损失函数最小值的时候，往往采用的是基于梯度的优化算法，但是许多函数的梯度为零的点并不是最小值，其可能是局部极小值点与鞍点。

事实上，在高维空间中，鞍点的数量比局部极小值点要多得多，从优化理论的角度去思考，可以利用 Hessian 矩阵分析：

- 当函数在零梯度位置处的Hessian矩阵的特征值全部为正值时，我们有该函数的局部最小值；

- 当函数在零梯度位置处的Hessian矩阵的特征值全部为负值时，我们有该函数的局部最大值；

- 当函数在零梯度位置处的Hessian矩阵的特征值为负值和正值时，我们有该函数的一个鞍点。

由于海森矩阵的维度比较高，所以遇到最多的应该是第三种情况。这就造成了目标函数的鞍点很多，而我们的优化算法很容易陷进去出不来的问题。

#### 梯度消失

关于梯度消失和梯度爆炸的问题，可以参见我之前写的博客《关于梯度消失和梯度爆炸》，当时这篇博客是我在阅读 GRU 神经网络的时候写就的，所以其侧重点在于介绍梯度分流，即一种类残差连接的方式来缓解梯度消失，对于其他方法有着简短的介绍可供参考。

上面的这两个优化挑战是神经网络优化算法的最主要的问题，但目前已经有很多方法来试图解决这一问题了，重参数化就是其中之一。

### 重参数化（Reparameterization）

#### 定义

和我一样，或许很多人刚开始接触重参数化是在学习 **变分推断** 的时候，但其实重参数化是个很广泛的概念，其本质是：将一个**难以优化的参数表达形式**转换为另一个**更易于优化的等价形式**，从而**避免梯度消失、加快收敛、或者使梯度可传播**。换句话说，就是换一种方式表达变量，使得优化过程更稳定或更容易。

当原始参数结构导致：

1. 梯度消失（如sigmoid深层网络）
2. 参数分布不可导（如随机变量 sampling）
3. 梯度不易传播（如 latent variable）

我们可以将模型参数进行重写，使得：

1. 梯度可以更有效地传播
2. 采样过程可导，便于反向传播（变分推断）
3. 激活函数或网络结构不易造成梯度缩小

#### 例子

常见的重参数化的例子有很多，比如：

1. **变分自编码器（VAE）中的重参数化技巧**

原始形式：

* 从一个高斯分布中采样隐变量：
  $$z \sim \mathcal{N}(\mu, \sigma^2)$$
* 但是采样是随机操作，**不可导**，梯度无法传到 $$\mu$$、$$\sigma$$

**重参数化方法**：

* 写成：
  $$z = \mu + \sigma \cdot \epsilon$$
  其中 $$\epsilon \sim \mathcal{N}(0, 1)$$ 是独立于模型参数的随机变量
* 这样，$$z$$ 是 $$\mu$$、$$\sigma$$ 的函数，就能通过反向传播训练网络

**结果**：采样过程变得可导了，梯度可以传播回去。

2. **深层网络中的激活函数替代**

原始网络使用 sigmoid 激活函数：

* 多层后梯度容易消失，因为 sigmoid 的梯度在两侧几乎为 0

**重参数化方法**：

* 用 ReLU、Leaky ReLU、GELU 等替代 sigmoid/tanh

**结果**：ReLU 等激活函数在正区间有常数梯度，不容易导致梯度消失。

3. **Batch Normalization 的视角**

BN 实际上也可以被看作是一种“重参数化”：

* 原始网络直接学习 $$x$$
* BN 网络学习的是标准化后的值 $$\hat{x} = \frac{x - \mu}{\sigma}$$，再乘以一个缩放参数 $$\gamma$$ 和偏置 $$\beta$$

**结果**：BN 可以缓解梯度消失、加快收敛。

4. **注意力机制中的重参数化**

在一些注意力网络中，例如 Performer / Mamba / Low-rank attention，也通过矩阵结构的“重构”或“分解”来替代原始高维度的操作，从而提升可训练性和数值稳定性。（Mamba 我有介绍过，另外两个我倒是不太熟，有空补一下吧）

总而言之，重参数化的定义比大多数人认知中的要广泛的多，可以说只要涉及到**对网络结构上的改变**，就是一种重参数化方法。不过这么说，重参数化似乎就失去其神秘的色彩了（）